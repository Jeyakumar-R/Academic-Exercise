{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be968ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a27383f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = '''AI involves many different fields like computer science mathematics linguistics psychology neuroscience and philosophy. Eventually researchers hope to create a general artificial intelligence which can solve many problems instead of focusing on just one. Researchers are also trying to create creative and emotional AI which can possibly empathize or create art. Many approaches and tools have been tried.\n",
    "\n",
    "Borrowing from the management literature Kaplan and Haenlein classify artificial intelligence into three different types of AI systems analytical human-inspired and humanized artificial intelligence. Analytical AI has only characteristics consistent with cognitive intelligence generating cognitive representation of the world and using learning based on past experience to inform future decisions. Human-inspired AI has elements from cognitive as well as emotional intelligence understanding in addition to cognitive elements also human emotions considering them in their decision making. Humanized AI shows characteristics of all types of competencies cognitive emotional and social intelligence able to be self-conscious and self-aware in interactions with others'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fee0fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9151492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI involves many different fields like computer science mathematics linguistics psychology neuroscience and philosophy.', 'Eventually researchers hope to create a general artificial intelligence which can solve many problems instead of focusing on just one.', 'Researchers are also trying to create creative and emotional AI which can possibly empathize or create art.', 'Many approaches and tools have been tried.', 'Borrowing from the management literature Kaplan and Haenlein classify artificial intelligence into three different types of AI systems analytical human-inspired and humanized artificial intelligence.', 'Analytical AI has only characteristics consistent with cognitive intelligence generating cognitive representation of the world and using learning based on past experience to inform future decisions.', 'Human-inspired AI has elements from cognitive as well as emotional intelligence understanding in addition to cognitive elements also human emotions considering them in their decision making.', 'Humanized AI shows characteristics of all types of competencies cognitive emotional and social intelligence able to be self-conscious and self-aware in interactions with others']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b6e815cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'addition', 'ai', 'all', 'also', 'analytical', 'and', 'approaches', 'are', 'art', 'artificial', 'as', 'aware', 'based', 'be', 'been', 'borrowing', 'can', 'characteristics', 'classify', 'cognitive', 'competencies', 'computer', 'conscious', 'considering', 'consistent', 'create', 'creative', 'decision', 'decisions', 'different', 'elements', 'emotional', 'emotions', 'empathize', 'eventually', 'experience', 'fields', 'focusing', 'from', 'future', 'general', 'generating', 'haenlein', 'has', 'have', 'hope', 'human', 'humanized', 'in', 'inform', 'inspired', 'instead', 'intelligence', 'interactions', 'into', 'involves', 'just', 'kaplan', 'learning', 'like', 'linguistics', 'literature', 'making', 'management', 'many', 'mathematics', 'neuroscience', 'of', 'on', 'one', 'only', 'or', 'others', 'past', 'philosophy', 'possibly', 'problems', 'psychology', 'representation', 'researchers', 'science', 'self', 'shows', 'social', 'solve', 'systems', 'the', 'their', 'them', 'three', 'to', 'tools', 'tried', 'trying', 'types', 'understanding', 'using', 'well', 'which', 'with', 'world']\n",
      "(8, 102)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "07b66153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>addition</th>\n",
       "      <th>ai</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>analytical</th>\n",
       "      <th>and</th>\n",
       "      <th>approaches</th>\n",
       "      <th>are</th>\n",
       "      <th>art</th>\n",
       "      <th>...</th>\n",
       "      <th>tools</th>\n",
       "      <th>tried</th>\n",
       "      <th>trying</th>\n",
       "      <th>types</th>\n",
       "      <th>understanding</th>\n",
       "      <th>using</th>\n",
       "      <th>well</th>\n",
       "      <th>which</th>\n",
       "      <th>with</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267678</td>\n",
       "      <td>0.267678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207983</td>\n",
       "      <td>0.416207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416207</td>\n",
       "      <td>0.416207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185831</td>\n",
       "      <td>0.221606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>0.109469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>0.219065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185941</td>\n",
       "      <td>0.092917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.212984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106431</td>\n",
       "      <td>0.212984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178497</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       able  addition        ai       all      also  analytical       and  \\\n",
       "0  0.000000  0.000000  0.145937  0.000000  0.000000    0.000000  0.145937   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.133762  0.000000  0.224335    0.000000  0.133762   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  0.207983   \n",
       "4  0.000000  0.000000  0.110803  0.000000  0.000000    0.185831  0.221606   \n",
       "5  0.000000  0.000000  0.109469  0.000000  0.000000    0.183594  0.109469   \n",
       "6  0.000000  0.185941  0.092917  0.000000  0.155833    0.000000  0.000000   \n",
       "7  0.212984  0.000000  0.106431  0.212984  0.000000    0.000000  0.212861   \n",
       "\n",
       "   approaches       are       art  ...     tools     tried    trying  \\\n",
       "0    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2    0.000000  0.267678  0.267678  ...  0.000000  0.000000  0.267678   \n",
       "3    0.416207  0.000000  0.000000  ...  0.416207  0.416207  0.000000   \n",
       "4    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "5    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "6    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "7    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "      types  understanding     using      well     which      with     world  \n",
       "0  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000       0.000000  0.000000  0.000000  0.218074  0.000000  0.000000  \n",
       "2  0.000000       0.000000  0.000000  0.000000  0.224335  0.000000  0.000000  \n",
       "3  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.185831       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5  0.000000       0.000000  0.219065  0.000000  0.000000  0.183594  0.219065  \n",
       "6  0.000000       0.185941  0.000000  0.185941  0.000000  0.000000  0.000000  \n",
       "7  0.178497       0.000000  0.000000  0.000000  0.000000  0.178497  0.000000  \n",
       "\n",
       "[8 rows x 102 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "vector = X\n",
    "df = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2b0c79ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\JEYA KUMAR\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\JEYA KUMAR\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "62615518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0a528293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_list(doc_text):\n",
    "    tokens = nltk.word_tokenize(doc_text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e3bb6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_stemmer(token_list):\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmed = []\n",
    "    for words in token_list:\n",
    "        stemmed.append(ps.stem(words))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "89d9a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc_text):\n",
    "    cleaned_text = []\n",
    "    for words in doc_text:\n",
    "        if words not in stop_words:\n",
    "            cleaned_text.append(words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8cfae923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD TOKENS:\n",
      "['Eventually', 'researchers', 'hope', 'to', 'create', 'a', 'general', 'artificial', 'intelligence', 'which', 'can', 'solve', 'many', 'problems', 'instead', 'of', 'focusing', 'on', 'just', 'one', '.']\n",
      "\n",
      "AFTER REMOVING STOPWORDS:\n",
      "['Eventually', 'researchers', 'hope', 'create', 'general', 'artificial', 'intelligence', 'solve', 'many', 'problems', 'instead', 'focusing', 'one', '.']\n",
      "\n",
      "AFTER PERFORMING THE WORD STEMMING::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['eventu',\n",
       " 'research',\n",
       " 'hope',\n",
       " 'creat',\n",
       " 'gener',\n",
       " 'artifici',\n",
       " 'intellig',\n",
       " 'solv',\n",
       " 'mani',\n",
       " 'problem',\n",
       " 'instead',\n",
       " 'focus',\n",
       " 'one',\n",
       " '.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = get_tokenized_list(corpus[1])\n",
    "print(\"WORD TOKENS:\")\n",
    "print(tokens)\n",
    "doc_text = remove_stopwords(tokens)\n",
    "print(\"\\nAFTER REMOVING STOPWORDS:\")\n",
    "print(doc_text)\n",
    "print(\"\\nAFTER PERFORMING THE WORD STEMMING::\")\n",
    "doc_text = word_stemmer(doc_text)\n",
    "doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ad602878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eventu research hope creat gener artifici intellig solv mani problem instead focus one .'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = ' '.join(doc_text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a2141805",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = []\n",
    "for doc in corpus:\n",
    "    tokens = get_tokenized_list(doc)\n",
    "    doc_text = remove_stopwords(tokens)\n",
    "    doc_text  = word_stemmer(doc_text)\n",
    "    doc_text = ' '.join(doc_text)\n",
    "    cleaned_corpus.append(doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5ab4bc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai involv mani differ field like comput scienc mathemat linguist psycholog neurosci philosophi .',\n",
       " 'eventu research hope creat gener artifici intellig solv mani problem instead focus one .',\n",
       " 'research also tri creat creativ emot ai possibl empath creat art .',\n",
       " 'mani approach tool tri .',\n",
       " 'borrow manag literatur kaplan haenlein classifi artifici intellig three differ type ai system analyt human-inspir human artifici intellig .',\n",
       " 'analyt ai characterist consist cognit intellig gener cognit represent world use learn base past experi inform futur decis .',\n",
       " 'human-inspir ai element cognit well emot intellig understand addit cognit element also human emot consid decis make .',\n",
       " 'human ai show characterist type compet cognit emot social intellig abl self-consci self-awar interact other']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6ce73d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abl', 'addit', 'ai', 'also', 'analyt', 'approach', 'art', 'artifici', 'awar', 'base', 'borrow', 'characterist', 'classifi', 'cognit', 'compet', 'comput', 'consci', 'consid', 'consist', 'creat', 'creativ', 'decis', 'differ', 'element', 'emot', 'empath', 'eventu', 'experi', 'field', 'focus', 'futur', 'gener', 'haenlein', 'hope', 'human', 'inform', 'inspir', 'instead', 'intellig', 'interact', 'involv', 'kaplan', 'learn', 'like', 'linguist', 'literatur', 'make', 'manag', 'mani', 'mathemat', 'neurosci', 'one', 'other', 'past', 'philosophi', 'possibl', 'problem', 'psycholog', 'represent', 'research', 'scienc', 'self', 'show', 'social', 'solv', 'system', 'three', 'tool', 'tri', 'type', 'understand', 'use', 'well', 'world']\n",
      "(8, 74)\n"
     ]
    }
   ],
   "source": [
    "vectorizerX = TfidfVectorizer()\n",
    "vectorizerX.fit(cleaned_corpus)\n",
    "doc_vector = vectorizerX.transform(cleaned_corpus)\n",
    "print(vectorizerX.get_feature_names())\n",
    "\n",
    "print(doc_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d909bfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>addit</th>\n",
       "      <th>ai</th>\n",
       "      <th>also</th>\n",
       "      <th>analyt</th>\n",
       "      <th>approach</th>\n",
       "      <th>art</th>\n",
       "      <th>artifici</th>\n",
       "      <th>awar</th>\n",
       "      <th>base</th>\n",
       "      <th>...</th>\n",
       "      <th>solv</th>\n",
       "      <th>system</th>\n",
       "      <th>three</th>\n",
       "      <th>tool</th>\n",
       "      <th>tri</th>\n",
       "      <th>type</th>\n",
       "      <th>understand</th>\n",
       "      <th>use</th>\n",
       "      <th>well</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160536</td>\n",
       "      <td>0.269239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556813</td>\n",
       "      <td>0.466653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240975</td>\n",
       "      <td>0.240975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236044</td>\n",
       "      <td>0.117954</td>\n",
       "      <td>0.197823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236044</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.253685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        abl     addit        ai      also    analyt  approach       art  \\\n",
       "0  0.000000  0.000000  0.147516  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.160536  0.269239  0.000000  0.000000  0.321257   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.556813  0.000000   \n",
       "4  0.000000  0.000000  0.120418  0.000000  0.201956  0.000000  0.000000   \n",
       "5  0.000000  0.000000  0.127065  0.000000  0.213104  0.000000  0.000000   \n",
       "6  0.000000  0.236044  0.117954  0.197823  0.000000  0.000000  0.000000   \n",
       "7  0.253685  0.000000  0.126769  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   artifici      awar      base  ...      solv    system     three      tool  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.256838  0.000000  0.000000  ...  0.306461  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.556813   \n",
       "4  0.403913  0.000000  0.000000  ...  0.000000  0.240975  0.240975  0.000000   \n",
       "5  0.000000  0.000000  0.254277  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.253685  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        tri      type  understand       use      well     world  \n",
       "0  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.269239  0.000000    0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.466653  0.000000    0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.201956    0.000000  0.000000  0.000000  0.000000  \n",
       "5  0.000000  0.000000    0.000000  0.254277  0.000000  0.254277  \n",
       "6  0.000000  0.000000    0.236044  0.000000  0.236044  0.000000  \n",
       "7  0.000000  0.212608    0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[8 rows x 74 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(doc_vector.toarray(), columns=vectorizerX.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2c930685",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'computer science'\n",
    "query = get_tokenized_list(query)\n",
    "query = remove_stopwords(query)\n",
    "q = []\n",
    "for w in word_stemmer(query):\n",
    "    q.append(w)\n",
    "q = ' '.join(q)\n",
    "q\n",
    "query_vector = vectorizerX.transform([q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eaf41a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosineSimilarities = cosine_similarity(doc_vector,query_vector).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0e38af0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 7 6 5 4 3 2 1]\n",
      "['ai involv mani differ field like comput scienc mathemat linguist psycholog neurosci philosophi .']\n",
      "['human ai show characterist type compet cognit emot social intellig abl self-consci self-awar interact other']\n",
      "['human-inspir ai element cognit well emot intellig understand addit cognit element also human emot consid decis make .']\n",
      "['analyt ai characterist consist cognit intellig gener cognit represent world use learn base past experi inform futur decis .']\n",
      "['borrow manag literatur kaplan haenlein classifi artifici intellig three differ type ai system analyt human-inspir human artifici intellig .']\n",
      "['mani approach tool tri .']\n",
      "['research also tri creat creativ emot ai possibl empath creat art .']\n",
      "['eventu research hope creat gener artifici intellig solv mani problem instead focus one .']\n"
     ]
    }
   ],
   "source": [
    "related_docs_indices = cosineSimilarities.argsort()[:-10:-1]\n",
    "print(related_docs_indices)\n",
    "\n",
    "for i in related_docs_indices:\n",
    "    data = [cleaned_corpus[i]]\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0852733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
